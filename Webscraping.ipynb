{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscrapping \n",
    "- Always check the robots.txt file of the target website that tells robots which page not to crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target URL to scrap \n",
    "url = \"https://www.goibibo.com/hotels/5-star-hotels-in-lucknow-cg/\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers= { }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=requests.request(\"GET\",url)\n",
    "response\n",
    "type(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse - syntatical analysis - understand the symbols , strings and mke it in usable data form and give information\n",
    "# Beautiful Soup is a Python library for getting data out of HTML, XML, and other markup languages. \n",
    "data=BeautifulSoup(response.text,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all sections with specific class name \n",
    "cards_data=data.find_all('div',attrs={'class','HotelCardstyles__WrapperSectionMetaDiv-sc-1s80tyk-2 cPtMpq'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of cards found:  10\n"
     ]
    }
   ],
   "source": [
    "#Totat number of cards found \n",
    "print(\"Total number of cards found: \", len(cards_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for cards in cards_data:\n",
    "    #print(cards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clarks Avadh 1877\n",
      "Ramada by Wyndham Lucknow 3014\n",
      "Hyatt Regency Lucknow 5099\n",
      "The Piccadily 4126\n",
      "Radisson Lucknow City Center 2275\n",
      "Renaissance Lucknow Hotel 6799\n",
      "Novotel Lucknow Gomti Nagar 3549\n",
      "Golden Tulip Hotel Lucknow 2055\n",
      "Taj Mahal, Lucknow 8093\n",
      "Fortune Park BBD-Member ITC Hotel Group 4300\n"
     ]
    }
   ],
   "source": [
    "# Extract the hotel name and price per room \n",
    "for cards in cards_data:\n",
    "    \n",
    "    #get the hotel name , found in a tag , only ine tag if this kind for each card \n",
    "    hotel_name = cards.find('a')\n",
    "    \n",
    "    #get the room price , found in p tag , more than one tag so need to mention the class name along with the tag \n",
    "    room_price = cards.find('p',attrs={'class':'HotelCardstyles__CurrentPrice-sc-1s80tyk-27 ieJkCi'})\n",
    "    print(hotel_name.text,room_price.text)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So now final is to get this into CSV or JSON file :\n",
    "# Creating a list to store the data \n",
    "scraped_data= []\n",
    "\n",
    "for cards in cards_data:\n",
    "    \n",
    "    # initialize the dictionary\n",
    "    card_details = {}\n",
    "    \n",
    "    #Get the hotel name \n",
    "    hotel_name = cards.find('a')\n",
    "    \n",
    "    #Get the room price\n",
    "    room_price = cards.find('p',attrs={'class':'HotelCardstyles__CurrentPrice-sc-1s80tyk-27 ieJkCi'})\n",
    "   \n",
    "    #Add data to dictionary\n",
    "    card_details['hotel_name']=hotel_name.text\n",
    "    card_details['room_price']=room_price.text\n",
    "    scraped_data.append(card_details)  #This is the list of dictionaries , adding dictionaries to the list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'hotel_name': 'Clarks Avadh', 'room_price': '1877'},\n",
       " {'hotel_name': 'Ramada by Wyndham Lucknow', 'room_price': '3014'},\n",
       " {'hotel_name': 'Hyatt Regency Lucknow', 'room_price': '5099'},\n",
       " {'hotel_name': 'The Piccadily', 'room_price': '4126'},\n",
       " {'hotel_name': 'Radisson Lucknow City Center', 'room_price': '2275'},\n",
       " {'hotel_name': 'Renaissance Lucknow Hotel', 'room_price': '6799'},\n",
       " {'hotel_name': 'Novotel Lucknow Gomti Nagar', 'room_price': '3549'},\n",
       " {'hotel_name': 'Golden Tulip Hotel Lucknow', 'room_price': '2055'},\n",
       " {'hotel_name': 'Taj Mahal, Lucknow', 'room_price': '8093'},\n",
       " {'hotel_name': 'Fortune Park BBD-Member ITC Hotel Group',\n",
       "  'room_price': '4300'}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataframe from the list of dictionaries \n",
    "final= pd.DataFrame.from_dict(scraped_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the scraped data as CSV File \n",
    "final.to_csv('LKO_5Star_hotels.csv',index=False,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single web-page scrapping can be done using some commands but for multiple pages it is not easy , have to write the script\n",
    "# Scrap website URLs and email IDs \n",
    "# Marketing Teams scrap Email IDs in bulk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web Scraping URLs and Email IDs \n",
    "\n",
    "#Importing required libraries \n",
    "import urllib.request \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#URL to Scrap \n",
    "Logistic = \"https://dlca.logcluster.org/display/public/DLCA/4.1+Nepal+Government+Contact+List\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query the website and get the response stored in the variable page \n",
    "PageR=urllib.request.urlopen(Logistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parse the HTML using Beautiful Soup \n",
    "soup = BeautifulSoup(PageR,features='html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Page Scraped!!!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PageR.url ,type(PageR)\n",
    "print('\\n\\nPage Scraped!!!\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title of the page is :  4.1 Nepal Government Contact List - Logistics Capacity Assessment - Digital Logistics Capacity Assessments\n"
     ]
    }
   ],
   "source": [
    "print(\"Title of the page is : \", soup.title.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the URLS in page\n"
     ]
    }
   ],
   "source": [
    "print(\"All the URLS in page\")\n",
    "All_U=soup.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of URLs present =  108\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of URLs present = \",len(All_U))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last five urls in the page are :\n",
      "http://www.atlassian.com/c/conf/11460\n",
      "http://www.atlassian.com/software/confluence\n",
      "https://support.atlassian.com/help/confluence\n",
      "http://www.atlassian.com/about/connected.jsp?s_kwcid=Confluence-stayintouch\n",
      "http://www.atlassian.com/\n"
     ]
    }
   ],
   "source": [
    "print(\"Last five urls in the page are :\")\n",
    "\n",
    "if len(All_U)>5:\n",
    "    last_5=All_U[len(All_U)-5:] #You want only last five URLS from this list , start point and end will happen\n",
    "\n",
    "\n",
    "for url in last_5:\n",
    "    print(url.get('href'))  # Get the exact link from the hyperlink reference \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"http://www.atlassian.com/c/conf/11460\">Evaluate Confluence today</a>,\n",
       " <a class=\"hover-footer-link\" href=\"http://www.atlassian.com/software/confluence\" rel=\"nofollow\">Atlassian Confluence</a>,\n",
       " <a class=\"hover-footer-link\" href=\"https://support.atlassian.com/help/confluence\" rel=\"nofollow\">Report a bug</a>,\n",
       " <a class=\"hover-footer-link\" href=\"http://www.atlassian.com/about/connected.jsp?s_kwcid=Confluence-stayintouch\" rel=\"nofollow\">Atlassian News</a>,\n",
       " <a href=\"http://www.atlassian.com/\" rel=\"nofollow\">Atlassian</a>]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of Email IDs present are : 27\n"
     ]
    }
   ],
   "source": [
    "emails=[]    #List to store all the email IDs , hyperlink reference\n",
    "\n",
    "for url in All_U:\n",
    "    if(str(url.get('href')).find('@')>0):\n",
    "        emails.append(url.get('href'))         #Store all the href of email IDs\n",
    "        \n",
    "print(\"The total number of Email IDs present are :\",len(emails))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Some of the Email IDs are :\n",
      "\n",
      "\n",
      "['mailto:info@nepal.gov.np', 'mailto:info@moad.gov.np', 'mailto:info@mod.gov.np', 'mailto:info@mohp.gov.np', 'mailto:gunaso@moha.gov.np']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\nSome of the Email IDs are :\\n\\n\")\n",
    "print(emails[:5]) #Just get five email IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrap Images in Python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web Scraping - Scrap Images \n",
    "# Importing required libraries \n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target URL to scrap \n",
    "Iurl = \"https://www.goibibo.com/hotels/5-star-hotels-in-lucknow-cg/\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res=requests.request(\"GET\",Iurl)\n",
    "res\n",
    "type(res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beautiful Soup is a Python library for getting data out of HTML, XML, and other markup languages. \n",
    "Data=BeautifulSoup(res.text,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Images : 68\n"
     ]
    }
   ],
   "source": [
    "# Find all with the image tag \n",
    "images = Data.find_all('img',src=True)\n",
    "\n",
    "print(\"Number of Images :\",len(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<meta content=\"https://cdn1.goibibo.com/voy_mmt/t_g/htl-imgs/200701181826311360-f08f935e8ce311e9ac800242ac110003.jpg\" itemprop=\"image\"/>"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link=Data.find(itemprop=\"image\")\n",
    "link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for image in images:\n",
    " #   print(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://goibibo.ibcdn.com/styleguide/images/goLogo.png\n",
      "https://goibibo.ibcdn.com/styleguide/images/train_logo.png\n",
      "https://gos3.ibcdn.com/gotripe-new2x-1600243849.png\n",
      "https://goibibo.ibcdn.com/styleguide/images/goCash-header.png\n"
     ]
    }
   ],
   "source": [
    "#src part of your link contains the image format and it is as hyperlink\n",
    "#Select src tag\n",
    "image_src=[x['src'] for x in images]\n",
    "\n",
    "#This is to select only the jpg format\n",
    "images_src=[x for x in image_src if x.endswith('.png')]\n",
    "\n",
    "\n",
    "for image in images_src:\n",
    "    print(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7'"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_src[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_count = 1 \n",
    "for image in images_src:\n",
    "    with open('image_'+str(image_count)+'.gif','wb') as f:\n",
    "        res= requests.get(image)\n",
    "        f.write(res.content)\n",
    "    image_count=image_count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape data om Page load , load more buttom etc , inspect the page to find how to load the data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
